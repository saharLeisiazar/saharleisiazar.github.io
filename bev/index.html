<html lang="en">
<head>
    <title>bev</title>
    <meta name="description" content="Project page for Semi-supervised bird-eye-view map generation for exoskeletons using an RGB-D camera.">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
    <meta charset="utf-8">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="style.css" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>    
</head>
<body>


<!--Title-->    
<div class="container" style="text-align:center; padding:2rem 15px">
    <div class="row" style="text-align:center">
        <h1>Semi-supervised bird-eye-view map generation for exoskeletons using an RGB-D camera</h1>
<!--         <h4>IEEE Robotics and Automation Letters 2025</h4> -->
    </div>
    <div class="row" style="text-align:center">
        <div class="col-xs-0 col-md-3"></div>
        <div class="col-xs-12 col-md-6">
        <h4>
    
            <!-- How to write authors and how to do we need links?-->
            <nobr>Sahar Leisiazar</nobr><sup></sup> &emsp;
            <nobr>Behzad Peykari</nobr><sup></sup> &emsp; 
            <nobr>Siamak Arzanpour</nobr><sup></sup> &emsp;
            <nobr>Farshid Najafi</nobr><sup></sup> </a>&emsp;
            <nobr>Edward J. Park</nobr><sup></sup></a>&emsp; -->

        </h4>


        <p><em><center>School of Mechatronics System Engineering, Simon Fraser University, Surrey, BC, Canada  &emsp;</center></em></p>
<!--         <p><em><center>School of Computing Science, Simon Fraser University, Burnaby, BC, Canada  &emsp;</center></em></p> -->
        </div>
        <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
        <a href="" style="color:inherit">
            <i class="fa fa-file-pdf-o fa-4x"></i></a> 
        </div>

    </div>
</div>





<!--Links-->
<div class="container" style="text-align:center; padding:1rem">
    <h3 style="text-align:center; padding-top:1rem; padding-bottom: 2em">
      <a class="label label-info" href="resrc/paper.pdf">Paper</a>
      <a class="label label-info" href="https://github.com/saharLeisiazar/Follow_ahead_reaction">Code</a>
      <a class="label label-info" href="https://youtu.be/hzJZIZihSKI">Video</a>

    </h3>

<!--     <div class="row" style="text-align:center; padding: 1rem">
          <div class="col-sm-6">
              <img src="resrc/first.drawio.png" alt="dti.png"  class="text-center" style="width: 100%; max-width: 1100px">
            </div>
            <div class="col-sm-6">
              <img src="resrc/tree_expansion.drawio.png" alt="deep_tsf.png"  class="text-center" style="width: 60%; max-width: 1100px">
            </div>
    </div>

      <div class="row" style="text-align:center; padding:1rem">
            <div class="col-sm-6">
              <p><h5><center>Real-world experiment showcasing the robot's ability to follow a human from the front while avoiding obstacles. The figure illustrates an example of the tree expansion process, with blue and red arrows representing the potential moves for the robot and human, respectively. The robot expands the tree and selects nodes that allow it to maintain a position in front of the human, while eliminating branches that could lead to collisions with obstacles.</center></h5></p>
            </div>
            <div class="col-sm-6">
              <p><h5><center>Tree expansion in MCTS: Blue nodes represent possible future positions of the robot, while red nodes indicate potential future positions of the human. The letters "L", "R", "S", and "F" on the edges of the tree represent the actions: Left, Right, Straight, and Fast, respectively.</center></h5></p>
            </div>
      </div> -->

    



<div class="container">
    <h3>Abstract</h3>
    <hr/>
    <p>
        We present a novel mapping approach for lower-limb exoskeletons that generates real-time, robot-centric bird’s-eye view (BEV) occupancy maps to support safe and efficient local navigation. This work focuses on a self-balancing wearable humanoid exoskeleton, where BEV mapping is essential for enabling autonomous balance control, footstep planning, and adaptive navigation in complex, real-world environments. The proposed method explicitly incorporates camera motion alongside RGB-D observations to improve mapping accuracy under the dynamic conditions introduced by leg-mounted sensors. To meet the computational constraints of embedded platforms, the model is optimized for real-time operation and can effectively track dynamic elements such as moving pedestrians. We further introduce a semi-supervised framework that combines simulation-based supervised training with unsupervised learning on real-world data, enabling robust generalization despite limited ground-truth labels. Experiments in both simulated and real environments confirm that the model remains robust to the exoskeleton’s motion as well as various sources of environmental noise. 
    </p>



    <h3>Approach</h3>
    <hr/>
    
    <p>In this paper, 
    
    </p>


<!-- <img src="resrc/modules.png" alt="dti.png"  class="text-center" style="width: 65%; max-width: 1100px"> -->


    
<!--       <div class="row" style="text-align:center; padding:1rem">
        <div class="col-sm-6">
          <img src="resrc/algorithm.png" alt="dti.png"  class="text-center" style="width: 85%; max-width: 1100px">
        </div>
        <div class="col-sm-6">

          <p>In this work, Monte Carlo Tree Search (MCTS) is employed to determine the optimal action for a robot to follow a human from the front. The algorithm involves four key steps: selection, expansion, simulation, and backpropagation. 
In the selection stage, the process starts at the root node, representing the current poses of the human and the robot. From this point, the algorithm computes the Upper Confidence Bound (UCB) value for each child node and selects the child with the highest value, continuing this selection process until it reaches a leaf node.
In this work, we modified the standard UCB approach by incorporating the probability of selecting each node. 
The parameters involved in the UCB equation include the value of each node $(V)$, the number of times a node $(n)$ and its parent $(n_p)$ have been selected, and the probability of selecting a node $(P)$. 
The value of each node is derived from the state evaluation module (RL model), while the probability is obtained from the human future positioning probabilities module (LSTM). 
              

              During the expansion stage, all possible next states for each action are simulated. If the selected leaf node is a robot node (blue node), the algorithm uses the human's actions and, if it is a human node (red node), it uses the robot's actions. 
In the evaluation stage, newly added leaf nodes are first assessed for safety to ensure they do not result in collisions with obstacles or the human. 
If a leaf node directs the robot toward an unsafe region, the algorithm removes that node from the tree and stops further expansion from that branch. 
Once safety is confirmed, the leaf nodes are evaluated using the state evaluation module. In this module, all the leaf nodes—both human and robot—are passed through the trained RL model to assign a value to each node. This value reflects how the robot's pose is close to the desired pose in relation to the human. In other words, the closer the robot is to being directly in front of the human at a certain distance, the higher the value assigned to the node.
Afterward, the human-related nodes are processed through the human future positioning probabilities module. This module assigns a probability to each human node based on the history of the human's positions over the past 3 seconds. It's important to note that all the robot nodes are assigned an equal probability of  1/6, where 6 represents the number of actions for the robot.
Finally, In the backpropagation stage, the value obtained for each leaf node is propagated back through the tree, updating the values of all parent nodes up to the root node.


At the end of expansion, which occurs over a $0.2$s interval, the immediate child node with the highest visit count $(n)$ is selected as the optimal action for the next time step. The algorithm then updates with the new poses of the human and robot and re-expands the tree for the subsequent time step.
<!--               <img src="resrc/r.png" alt="dti.png"  class="text-center" style="width: 80%; max-width: 1100px">               -->
        </div>
      </div>     -->
    

<p style="padding-top: 1em; padding-bottom: 2em">

    <h3>Experiments</h3>
    <div class="container" style="text-align:center; padding:1rem">

   <p> 
   </p>

    <hr/>
      <!-- first experiment -->
     

        <pre><p><h4><em>Map generation on exoskeleton</h4></p></pre>

        <p> 
        </p>
          
        <p style="padding-top: 1em; padding-bottom: 2em">


    <div class="row" style="text-align:center; padding: 1rem">
          <div class="col-sm-6">
              <img src="res/6.gif" alt="dti.png"  class="text-center" style="width: 100%; max-width: 1100px">
            </div>
            <div class="col-sm-6">
              <img src="res/5.gif" alt="deep_tsf.png"  class="text-center" style="width: 100%; max-width: 1100px">
            </div>
    </div>

      <div class="row" style="text-align:center; padding:1rem">
            <div class="col-sm-6">
              <p><h5><center> caption</center></h5></p>
            </div>
            <div class="col-sm-6">
              <p><h5><center> caption</center></h5></p>
            </div>
      </div>


    <div class="row" style="text-align:center; padding: 1rem">
          <div class="col-sm-6">
              <img src="res/4.gif" alt="dti.png"  class="text-center" style="width: 100%; max-width: 1100px">
            </div>
            <div class="col-sm-6">
              <img src="res/3.gif" alt="deep_tsf.png"  class="text-center" style="width: 100%; max-width: 1100px">
            </div>
    </div>

      <div class="row" style="text-align:center; padding:1rem">
            <div class="col-sm-6">
              <p><h5><center> caption</center></h5></p>
            </div>
            <div class="col-sm-6">
              <p><h5><center> caption</center></h5></p>
            </div>
      </div>


    <div class="row" style="text-align:center; padding: 1rem">
          <div class="col-sm-6">
              <img src="res/1.gif" alt="dti.png"  class="text-center" style="width: 100%; max-width: 1100px">
            </div>
            <div class="col-sm-6">
              <img src="res/7.gif" alt="deep_tsf.png"  class="text-center" style="width: 100%; max-width: 1100px">
            </div>
    </div>

      <div class="row" style="text-align:center; padding:1rem">
            <div class="col-sm-6">
              <p><h5><center> caption</center></h5></p>
            </div>
            <div class="col-sm-6">
              <p><h5><center> caption</center></h5></p>
            </div>
      </div>        
      </div>








    
      <!-- second experiment -->
    <pre><p><h4><em> Occupancy Map Accuracy Evaluation</h4></p></pre>

    <p>
    </p>

<!-- <img src="resrc/box_obstacle.png" alt="dti.png"  class="text-center" style="width: 55%; max-width: 1100px"> -->


  
  


    <p style="padding-top: 1em; padding-bottom: 2em">

      <h4 style="padding-top:0.5em">BibTeX</h4>
      If you find this work useful for your research, please cite:
      <div class="card">
        <div class="card-block">
          <pre class="card-text clickselect">
            @inproceedings{X,
            title={Adapting to Frequent Human Direction Changes in Autonomous Frontal Following Robots},
            author={Leisiazar, Sahar AND Rohani, Roozbeh  AND Park, Edward AND Lim, Angelica AND Chen, Mo},   
            booktitle={IEEE Robotics and Automation Letters},
            year={2025},
            }
          </pre>
        </div>
      </div>

</div>      







</body>
</html>
