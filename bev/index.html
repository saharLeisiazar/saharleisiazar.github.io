<html lang="en">
<head>
    <title>bev</title>
    <meta name="description" content="Project page for Semi-supervised bird-eye-view map generation for exoskeletons using an RGB-D camera.">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
    <meta charset="utf-8">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="style.css" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>    
</head>
<body>


<!--Title-->    
<div class="container" style="text-align:center; padding:2rem 15px">
    <div class="row" style="text-align:center">
        <h1>Semi-Supervised Bird’s-Eye-View Mapping for Self-Balancing Exoskeletons Using RGB-D Sensing</h1>
<!--         <h4>IEEE Robotics and Automation Letters 2025</h4> -->
    </div>
    <div class="row" style="text-align:center">
        <div class="col-xs-0 col-md-3"></div>
        <div class="col-xs-12 col-md-6">
        <h4>
    
            <!-- How to write authors and how to do we need links?-->
            <nobr>Sahar Leisiazar</nobr><sup></sup> &emsp;
            <nobr>Behzad Peykari</nobr><sup></sup> &emsp; 
            <nobr>Siamak Arzanpour</nobr><sup></sup> &emsp;
            <nobr>Farshid Najafi</nobr><sup></sup> </a>&emsp;
            <nobr>Edward J. Park</nobr><sup></sup></a>&emsp; 

        </h4>


        <p><em><center>School of Mechatronics System Engineering, Simon Fraser University, Surrey, BC, Canada  &emsp;</center></em></p>
        <p><em><center>Human in Motion Robotics Inc., Vancouver, Canada &emsp;</center></em></p>
        </div>
        <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
        <a href="" style="color:inherit">
            <i class="fa fa-file-pdf-o fa-4x"></i></a> 
        </div>

    </div>
</div>


<!--Links-->
<div class="container" style="text-align:center; padding:1rem">
    <h3 style="text-align:center; padding-top:1rem; padding-bottom: 2em">
      <a class="label label-info" href="https://saharleisiazar.github.io/bev">Paper</a>
      <a class="label label-info" href="https://saharleisiazar.github.io/bev">Code</a>
      <a class="label label-info" href="https://youtu.be/gEpJcAVKd1E">Video</a>

    </h3>


<div class="container">
    
      <div class="row" style="text-align:center; padding:1rem">
            <div class="col-sm-6">
              <img src="res/exo.png" alt="dti.png"  class="text-center" style="width: 65%; max-width: 1100px">
            </div>
            <div class="col-sm-6">
                <h1>Abstract</h1>
                <hr/>
                <p>
                    We present a novel mapping approach for lower-limb exoskeletons that generates real-time, robot-centric bird’s-eye view (BEV) occupancy maps to support safe and efficient 
                    local navigation. This work focuses on a self-balancing wearable humanoid exoskeleton, where BEV mapping is essential for enabling autonomous balance control, footstep 
                    planning, and adaptive navigation in complex, real-world environments. The proposed method explicitly incorporates camera motion alongside RGB-D observations to improve 
                    mapping accuracy under the dynamic conditions introduced by leg-mounted sensors. To meet the computational constraints of embedded platforms, the model is optimized for 
                    real-time operation and can effectively track dynamic elements such as moving pedestrians. We further introduce a semi-supervised framework that combines simulation-based 
                    supervised training with unsupervised learning on real-world data, enabling robust generalization despite limited ground-truth labels. Experiments in both simulated and 
                    real environments confirm that the model achieves fast inference (17 ms) with low memory consumption (600 MB). Moreover, the model remains robust to the exoskeleton’s 
                    motion as well as various sources of environmental noise.
                </p>
            </div>
      </div> 

    <h1 style="text-align: left;">Contributions</h1>
    <hr/>
    <ul style="text-align: left;">
      <li> We present, to the best of our knowledge, the first mapping module implemented on a lower-limb, self-balancing exoskeleton, capable of generating a real-time, robot-centric BEV occupancy map to support local and safe navigation</li>
      <li>The proposed method explicitly incorporates the camera’s motion, particularly the pitch angle, alongside RGB-D data to improve mapping accuracy in the presence of leg-mounted camera dynamics</li>
      <li>In contrast to conventional SLAM algorithms, the model operates in real-time, enabling dynamic objects mapping such as walking humansm</li>
      <li> We propose a semi-supervised training framework that combines supervised learning on simulation data with unsupervised learning on real-world observations, enabling the model to achieve accurate and robust occupancy map generation</li>  
    </ul>
        


    <h1 style="text-align: left;">Approach</h1>
    <hr/>

    <img src="res/model.png" alt="dti.png"  class="text-center" style="width: 90%; max-width: 1100px">
      <div class="row" style="text-align:center; padding:1rem">
          
      <p> We propose a learning-based approach for local map generation and train a model that takes three inputs: an RGB image, a depth image, and the camera’s pitch angle. Mounted on the 
          exoskeleton’s leg, the camera undergoes 3D rotational motion during walking, including roll, pitch, and yaw along the X, Y, and Z axes. Among these, the pitch angle has the most 
          significant impact on mapping accuracy, as it directly effects how obstacles are perceived and projected in the BEV map. To address this, our model jointly encodes features from 
          all three inputs and learns to predict the occupancy map representing the free and occupied areas in front of the robot. This design enables the system to account for camera motion 
          and produce accurate and real-time map. 


          The proposed model generates BEV maps using a three-module architecture: 
          <ul style="text-align: left;">
              <li> An RGB-depth feature extractor: employs two parallel ResNet-50 encoders—one for RGB images and one for depth images —producing fused feature maps.</li>
              <li> A pitch-aware conditioning module (PitchFiLM): encodes the leg-mounted camera’s pitch angle using sine and cosine components, generating channel-wise scaling and bias parameters to modulate the fused features. </li>
              <li> A convolutional decoder: progressively upsamples these conditioned features through five convolutional blocks with bilinear upsampling and dropout, ultimately producing an occupancy map via a sigmoid activation. </li>
          </ul>

       </p>
       <p>   
          The output represents a robot-centric, real-world-scaled map indicating obstacle presence for short-horizon navigation and real-time obstacle avoidance in dynamic environments for lower-limb exoskeletons.
          </p>
     
    </div>
  </div>     
    

<p style="padding-top: 1em; padding-bottom: 2em">

    <h1 style="text-align: left;">
        Experiments
    </h1>
    <div class="container" style="text-align:center; padding:1rem">

    <hr/>
      <!-- first experiment -->
     

        <pre><p><h4><em>Map Generation on the Exoskeleton</h4></p></pre>

        <p> The exoskeleton follows a cyclic gait in which one leg is in the stance phase while the other swings forward. Our algorithm generates the map only during the stance phase, using 
       RGB-D data from the camera mounted on the supporting leg. 
            The exoskeleton was teleoperated via joystick and manually guided toward obstacles to evaluate the performance of the map generation model.

        Each video includes:
          <ul style="text-align: left;">
            <li>A third-person view of the exoskeleton being manually teleoperated as it navigates among surrounding obstacles, </li>
            <li>The first-person camera image used as input to the model for map generation,</li>
            <li>The local, real-time BEV occupancy map predicted by our proposed method, with the robot’s position shown at the bottom center of the map,</li>
            <li>A comparative visualization of the map generated by RTAB-Map, which begins drifting early in each experiment and fails to capture dynamic obstacles, such as a walking person.</li>
          </ul>
        </p>
          
        <p style="padding-top: 1em; padding-bottom: 2em">


    <div class="row g-0" style="text-align:center; padding: 1rem">
      <div class="col-sm-6" style="border-right: 2px solid #ccc; border-bottom: 2px solid #ccc;">
        <div style="overflow: hidden; width: 100%; max-width: 1100px;">
          <img src="res/6.gif" alt="dti.png" 
               style="width: 105%; margin: -2% 0 0 -2%;">
        </div>
      </div>
      <div class="col-sm-6" style="border-bottom: 2px solid #ccc;">
        <div style="overflow: hidden; width: 100%; max-width: 1100px;">
          <img src="res/5.gif" alt="deep_tsf.png" 
               style="width: 105%; margin: -2% 0 0 -2%;">
        </div>
      </div>
    </div>



    <div class="row g-0" style="text-align:center; padding: 1rem">
      <div class="col-sm-6"  style="border-right: 2px solid #ccc; border-bottom: 2px solid #ccc;">
        <div style="overflow: hidden; width: 100%; max-width: 1100px;">
          <img src="res/4.gif" alt="dti.png" 
               style="width: 105%; margin: -2% 0 0 -2%;">
        </div>
      </div>
      <div class="col-sm-6" style="border-bottom: 2px solid #ccc;">
        <div style="overflow: hidden; width: 100%; max-width: 1100px;">
          <img src="res/3.gif" alt="deep_tsf.png" 
               style="width: 105%; margin: -2% 0 0 -2%;">
        </div>
      </div>
    </div>


    <div class="row g-0" style="text-align:center; padding: 1rem">
      <div class="col-sm-6"  style="border-right: 2px solid #ccc; border-bottom: 2px solid #ccc;">
        <div style="overflow: hidden; width: 100%; max-width: 1100px;">
          <img src="res/1.gif" alt="dti.png" 
               style="width: 105%; margin: -2% 0 0 -2%;">
        </div>
      </div>
      <div class="col-sm-6" style="border-bottom: 2px solid #ccc;">
        <div style="overflow: hidden; width: 100%; max-width: 1100px;">
          <img src="res/7.gif" alt="deep_tsf.png" 
               style="width: 105%; margin: -2% 0 0 -2%;">
        </div>
      </div>
    </div>


      

      </div>








    
      <!-- second experiment -->
    <pre><p><h4><em> Occupancy Map Accuracy Evaluation</h4></p></pre>


    <div class="row" style="text-align:center; padding:1rem">
            <div class="col-sm-6">
              <p>We used standard pixel-wise classification metrics to evaluate how well the predicted occupancy maps match the ground truth. The following metrics are used: 
              <ul style="text-align: left;">
                  <li> Precision: The fraction of predicted occupied pixels that are actually occupied in the ground truth.</li>
                  <li>Recall: The fraction of occupied pixels in the ground truth that are correctly predicted by the model.</li>
                  <li>F1-score: The harmonic mean of precision and recall, providing a balanced measure of both.</li>
              </ul>
            
              </p>

                <p> Each row in the table reflects obstacle distance from the robot while each column corresponds to a different pitch range of the camera mounted on the robot.</p>
            </div>
            <div class="col-sm-6">
                              
                <img src="res/exp1.png" alt="dti.png" class="text-center" style="width: 100%;">

            </div>
      </div> 




    <pre><p><h4><em> Impact of Noise and Brightness Variations on Occupancy Map Prediction</h4></p></pre>
      <div class="row" style="text-align:center; padding:1rem">
            <div class="col-sm-6">
              <p>We randomly selected 100 scenes and applied brightness changes to the RGB images and various noise models to the depth inputs.
                  A brightness scaling factor ranging from 0 to 1 was applied to the input RGB images, where 1 corresponds to full brightness and 0 represents complete darkness.
                  In the second set of experiments, we analyzed the model’s sensitivity to noise in the depth data. Here, the RGB images were kept unchanged (full brightness), while the depth maps were subjected to the following noise conditions.
              </p>

            </div>
          
            <div class="col-sm-6">
                              
                <img src="res/exp2.png" alt="dti.png" class="text-center" style="width: 100%;">

            </div>

          <img src="res/exp3.png" alt="dti.png" class="text-center" style="width: 80%;">
      </div> 

    


    <p style="text-align: left; padding-top: 1em; padding-bottom: 2em" >

<h4 style="padding-top:0.5em; text-align:left;">BibTeX</h4>
<p style="text-align:left;">
  If you find this work useful for your research, please cite:
</p>
<div class="card" style="text-align:left;">
  <div class="card-block">
    <pre class="card-text clickselect" style="text-align:left; margin:0;">
        @inproceedings{X,
          title={Semi-Supervised Bird’s-Eye-View Mapping for Self-Balancing Exoskeletons Using RGB-D Sensing},
          author={Leisiazar, Sahar AND Peykari, Behzad AND Arzanpour, Siamak AND Najafi, Farshid AND Park, Edward},   
          booktitle={IEEE International Conference on Robotics & Automation},
          year={2026},
        }
    </pre>
  </div>
</div>


</div>      







</body>
</html>
